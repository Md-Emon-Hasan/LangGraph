{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 92 document chunks\n",
      "\n",
      "üîç Query: What is chain-of-thought prompting?\n",
      "\n",
      "üìù Answer: <think>\n",
      "Okay, so I'm trying to understand what chain-of-thought prompting is. From the context provided, it seems like it's a technique used with large language models (LLMs) to help them handle complex tasks. Let me break this down.\n",
      "\n",
      "First, the context mentions that complex tasks usually involve many steps. An autonomous agent needs to plan ahead, which makes sense because without planning, it might get stuck or make mistakes. So, task decomposition is important here. The term \"chain of thought\" (CoT) is introduced as a standard prompting technique. It's by Wei et al. in 2022, so it's a relatively recent development.\n",
      "\n",
      "The idea is to instruct the model to \"think step by step.\" This makes the model use more test-time computation to break down hard tasks into smaller, simpler steps. That way, the model can manage each part individually, making the overall task less daunting. It also mentions that CoT transforms big tasks into multiple manageable tasks and provides insight into the model's thinking process. So, it's not just about getting the answer but understanding how the model arrives at it.\n",
      "\n",
      "Then there's the Tree of Thoughts (ToT) by Yao et al. in 2023, which extends CoT. Instead of just a single chain, ToT explores multiple reasoning possibilities at each step, creating a tree structure. This sounds like it allows the model to consider different paths or solutions, which could be useful for problems with multiple correct answers or when the best approach isn't immediately clear. The search process can be either breadth-first (BFS) or depth-first (DFS), and each state is evaluated, maybe using a classifier or majority vote. I'm not entirely sure how the evaluation works, but it probably helps in selecting the best path from the possibilities generated.\n",
      "\n",
      "The context also talks about how task decomposition can be done in different ways. One way is by using simple prompts like \"Steps for XYZ.\\n1.\" or asking for subgoals. Another is using task-specific instructions, like asking for a story outline when writing a novel. The third method involves human inputs, which might mean that a person helps break down the task into steps for the model.\n",
      "\n",
      "There's another approach called LLM+P by Liu et al. in 2023, which uses an external classical planner with PDDL. PDDL stands for Planning Domain Definition Language, which I think is a standard for describing planning problems. So, the LLM translates the problem into PDDL, uses a planner to generate a plan, and then translates that plan back into natural language. This seems useful in domains where such planners are available, like robotics, but maybe not so much in other areas where PDDL isn't common.\n",
      "\n",
      "Self-reflection is also mentioned as a vital aspect for autonomous agents. It allows them to improve by refining past decisions and correcting mistakes, which is important in real-world tasks where trial and error are part of the process.\n",
      "\n",
      "Putting this all together, chain-of-thought prompting is a way to make LLMs more effective at complex tasks by breaking them down into manageable steps. It's about instructing the model to think aloud, so to speak, and lay out its reasoning process. This not only helps in solving the task but also makes the model's decision-making more transparent.\n",
      "\n",
      "I'm a bit confused about how exactly the model is instructed to do this. Is it through specific prompts, or is it a setting in the model's configuration? The context says it's a prompting technique, so I think it's about the way you phrase your instructions to the model. For example, instead of just asking for the answer, you ask the model to outline the steps it would take to reach the answer.\n",
      "\n",
      "Also, the Tree of Thoughts seems like an advanced version of CoT. Instead of a linear chain, it branches out, allowing for more comprehensive exploration of possible solutions. This could be particularly useful in creative tasks or problem-solving where multiple approaches are valid.\n",
      "\n",
      "The mention of LLM+P using PDDL makes me think that this approach is more about integrating LLMs with existing planning tools. It might be more efficient in certain domains but less flexible in others where PDDL isn't applicable. So, chain-of-thought is more of a general-purpose technique, whereas LLM+P is niche but powerful where applicable.\n",
      "\n",
      "In summary, chain-of-thought prompting is a method to enhance LLM performance on complex tasks by having the model break down the task into smaller steps and articulate its reasoning process. This makes the model more effective and transparent in its problem-solving approach.\n",
      "</think>\n",
      "\n",
      "Chain-of-thought (CoT) prompting is a technique used to enhance the performance of large language models (LLMs) on complex tasks by instructing the model to break down tasks into smaller, manageable steps. This approach involves prompting the model to \"think step by step,\" allowing it to utilize more computational resources during the problem-solving process. By doing so, the model not only solves the task but also provides insight into its reasoning, making its decision-making\n",
      "\n",
      "üîó Sources:\n",
      "1. https://lilianweng.github.io/posts/2023-06-23-agent/: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated ta...\n",
      "2. https://lilianweng.github.io/posts/2023-06-23-agent/: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated ta...\n",
      "3. https://lilianweng.github.io/posts/2023-06-23-agent/: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated ta...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Query: Explain LLM adversarial attacks\n",
      "\n",
      "üìù Answer: <think>\n",
      "Okay, so I need to explain LLM adversarial attacks based on the provided context from Lil'Log. Let me start by understanding what the user is asking for. They want an explanation of adversarial attacks on Large Language Models, and they've provided a table of contents from a blog post by Lilian Weng. \n",
      "\n",
      "First, I should break down the structure of the blog post to organize my explanation. The blog has sections like Basics, Threat Model, Classification, Text Generation, White-box vs Black-box, Types of Attacks, Mitigation, and References. \n",
      "\n",
      "I think I should start by defining what adversarial attacks are in the context of LLMs. Maybe explain how they're different from traditional adversarial attacks in other ML models. Then, move on to the threat model, which probably outlines who the attackers are and what their goals might be. \n",
      "\n",
      "Next, the classification section likely categorizes attacks based on whether the model is used for classification or text generation. I should explain how attacks differ in these two scenarios. Then, the white-box vs black-box section probably discusses the level of access an attacker has. In white-box, the attacker knows the model's internals, while in black-box, they don't. \n",
      "\n",
      "The types of attacks section lists Token Manipulation, Gradient-based Attacks, Jailbreak Prompting, and Human/Model Red-teaming. I need to explain each of these. Token manipulation might involve changing input tokens to deceive the model. Gradient-based attacks use the model's gradients to craft adversarial examples. Jailbreak prompting is about bypassing the model's safety measures. Red-teaming involves humans or other models trying to find vulnerabilities.\n",
      "\n",
      "Then, the mitigation section probably talks about how to defend against these attacks, maybe touching on the saddle point problem and some robustness studies. \n",
      "\n",
      "I should make sure to cover each of these points in a clear, logical flow, starting from the basics and moving into more detailed explanations of each type of attack and how to mitigate them. I also need to keep the language accessible, avoiding too much jargon without explaining it.\n",
      "\n",
      "Wait, I should also consider the user's perspective. They might be someone new to LLMs and adversarial attacks, so I need to make the explanation thorough but not overly technical. Maybe include examples for each type of attack to make it clearer.\n",
      "\n",
      "I should also ensure that I'm not missing any key points from the provided context. The blog post seems comprehensive, so I need to touch on all the sections mentioned. \n",
      "\n",
      "Let me outline the structure:\n",
      "\n",
      "1. Introduction to LLMs and adversarial attacks\n",
      "2. Basics: Definition and how they apply to LLMs\n",
      "3. Threat Model: Who are the attackers and what are their goals\n",
      "4. Classification vs Text Generation attacks\n",
      "5. White-box vs Black-box attacks\n",
      "6. Types of attacks with explanations and examples\n",
      "7. Mitigation strategies\n",
      "8. Conclusion summarizing the importance of understanding these attacks\n",
      "\n",
      "I think that covers everything. Now, I'll proceed to write the explanation following this structure, making sure each section is concise and informative.\n",
      "</think>\n",
      "\n",
      "**Understanding Adversarial Attacks on Large Language Models (LLMs)**\n",
      "\n",
      "**Introduction:**\n",
      "Large Language Models (LLMs) have become integral to various applications, from text generation to question-answering. However, like other machine learning models, they are vulnerable to adversarial attacks. These attacks involve crafting inputs to deceive the model, leading to unintended behaviors. Understanding these attacks is crucial for developing robust LLMs.\n",
      "\n",
      "**Basics:**\n",
      "Adversarial attacks on LLMs involve manipulating inputs to cause the model to make mistakes. Unlike traditional attacks in machine learning, LLM attacks often target the model's language understanding and generation capabilities. Attackers exploit the model's patterns and biases to elicit specific responses.\n",
      "\n",
      "**Threat Model:**\n",
      "The threat model identifies potential attackers and their objectives. Attackers could range from malicious individuals to researchers testing model robustness. Their goals might include extracting sensitive information, bypassing content filters, or causing misinformation.\n",
      "\n",
      "**Classification vs. Text Generation Attacks:**\n",
      "- **Classification Attacks:** Target models used for text classification, such as sentiment analysis. Attackers might alter text to change the model's classification.\n",
      "- **Text Generation Attacks:** Aim at models generating text, like chatbots. Attackers might manipulate prompts to elicit harmful or unintended responses.\n",
      "\n",
      "**White-box vs. Black-box Attacks:**\n",
      "- **White-box Attacks:** Attackers have full access to the model's architecture and parameters, allowing precise crafting of adversarial examples.\n",
      "- **Black-box Attacks:** Attackers only have access to model outputs, requiring them to infer internal workings through trial and error.\n",
      "\n",
      "**Types of Attacks:**\n",
      "1. **Token Manipulation:** Altering input tokens (words or characters) to deceive the model. For example, adding typos or synonyms to mislead the model.\n",
      "2. **Gradient-based Attacks:** Using the model's gradients to create adversarial examples. This method is common in white-box attacks.\n",
      "3.\n",
      "\n",
      "üîó Sources:\n",
      "1. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: Adversarial Attacks on LLMs | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "2. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: Adversarial Attacks on LLMs | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "3. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: Adversarial Attacks on LLMs | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Query: Who is Yoshua Bengio?\n",
      "\n",
      "üìù Answer: <think>\n",
      "Okay, so I need to figure out who Yoshua Bengio is. I remember hearing the name before in the context of AI and machine learning, but I'm not exactly sure about the details. Let me start by breaking down the information given.\n",
      "\n",
      "First, the context mentions that Yoshua Bengio is a Canadian-French computer scientist and a pioneer in artificial neural networks and deep learning. That already gives me a hint that he's a big name in AI. I know that neural networks are a fundamental part of machine learning, so being a pioneer there means he's done some groundbreaking work.\n",
      "\n",
      "He's a professor at the Universit√© de Montr√©al and the scientific director of MILA, which is an AI institute. I think MILA stands for Montreal Institute for Learning Algorithms. That makes sense because Montreal is known for its strong AI research community. Being the scientific director there probably means he's leading a lot of research initiatives.\n",
      "\n",
      "The summary also mentions that he received the 2018 ACM A.M. Turing Award along with Geoffrey Hinton and Yann LeCun. I've heard of the Turing Award before; it's like the Nobel Prize of Computing. So that's a huge deal. They were awarded for their foundational work on deep learning. I remember that deep learning involves multiple layers of neural networks, which have been instrumental in advancements like image and speech recognition.\n",
      "\n",
      "It's interesting that they're referred to as the \"Godfathers of AI.\" That nickname suggests they've had a significant and lasting impact on the field. I can see why, given their contributions to deep learning, which has revolutionized AI in recent years.\n",
      "\n",
      "The context also notes that Bengio is the most-cited computer scientist globally, both by total citations and by h-index. The h-index measures the productivity and citation impact of a researcher. Being the most-cited indicates that his work is highly influential and widely referenced. Additionally, he's the most-cited living scientist across all fields by total citations, which is an impressive feat.\n",
      "\n",
      "In 2024, TIME Magazine included him in their list of the 100 most influential people in the world. That's a significant recognition beyond the academic community, showing that his work has broader societal impact.\n",
      "\n",
      "Looking at the second page about Samy Bengio, it seems he's Yoshua's brother. Samy is also a computer scientist, working at Apple as a senior director in AI and Machine Learning Research. That explains the familial connection but also shows that the Bengio family has a strong presence in AI research.\n",
      "\n",
      "Putting this all together, Yoshua Bengio is a leading figure in AI, particularly in deep learning. His work with Hinton and LeCun has laid the groundwork for many advancements in the field. His numerous accolades, including the Turing Award and being named by TIME Magazine, highlight his influence both academically and in the public eye.\n",
      "\n",
      "I'm curious about specific contributions he's made beyond the general deep learning work. Maybe he's published seminal papers or developed specific algorithms. Also, his role at MILA suggests he's involved in training the next generation of AI researchers, which is important for the future of the field.\n",
      "\n",
      "I wonder how his work has impacted practical applications. Deep learning is used in everything from facial recognition to natural language processing. His foundational work must have enabled many of these technologies. It's also interesting that he's based in Montreal, which has become a hub for AI research, thanks in part to his and others' efforts.\n",
      "\n",
      "I should also consider the timeline of his work. The Turing Award was in 2018, so his contributions were recognized about a decade after the resurgence of deep learning in the early 2010s. That makes sense because the impact of deep learning became more apparent around that time with advancements in computing power and data availability.\n",
      "\n",
      "In summary, Yoshua Bengio is a key figure in AI, known for his work in deep learning, and has received numerous accolades for his contributions. His influence extends beyond academia into industry and public recognition, solidifying his status as one of the most important researchers in the field.\n",
      "</think>\n",
      "\n",
      "Yoshua Bengio is a prominent Canadian-French computer scientist and a leading figure in the field of artificial intelligence, particularly known for his pioneering work in deep learning and artificial neural networks. Here are the key points that summarize his contributions and impact:\n",
      "\n",
      "1. **Academic and Research Contributions**:\n",
      "   - **Pioneer in Deep Learning**: Bengio, along with Geoffrey Hinton and Yann LeCun, is recognized for foundational work in deep learning, which has revolutionized AI with applications in image and speech recognition.\n",
      "   - **Leadership Roles**: He is a professor at the Universit√© de Montr√©al and the scientific director of MILA (Montreal Institute for Learning Algorithms), a hub for AI research.\n",
      "\n",
      "2. **Awards and Recognition**:\n",
      "   - **2018 ACM A.M. Turing Award**: Awarded for his contributions to deep learning, often referred to as the \"Nobel Prize of Computing.\"\n",
      "   - **Most-Cited Scientist**:\n",
      "\n",
      "üîó Sources:\n",
      "1. unknown: Page: Yoshua Bengio\n",
      "Summary: Yoshua Bengio  (born March 5, 1964) is a Canadian-French computer scien...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Query: Latest papers on transformer architectures\n",
      "\n",
      "üìù Answer: <think>\n",
      "Okay, so I'm trying to find the latest papers on transformer architectures. I remember that transformers were introduced a few years back and have become really popular in NLP, but they're also being used in other areas like computer vision now. I'm not super familiar with all the recent advancements, so I need to figure out where to look and what to focus on.\n",
      "\n",
      "First, I should probably check the major AI and machine learning conferences. Places like NeurIPS, ICML, IJCAI, and maybe even CVPR since transformers are used in vision tasks too. These conferences usually have the cutting-edge research. Also, arXiv.org is a good resource because many papers are posted there before they're published in conferences.\n",
      "\n",
      "I remember hearing about Vision Transformers (ViT) a while back, so I should look into any follow-up papers or improvements on that. Maybe there are variants like Swin Transformer or T2T-ViT. These might have been published in the last couple of years, so I should check if there are any new versions or applications of these models.\n",
      "\n",
      "Another angle is efficiency. With the move towards deploying models on edge devices, there's probably research on making transformers more efficient. This could involve model compression, quantization, or pruning. Maybe something like the Arch-Net paper mentioned earlier, which talks about model distillation and quantization for deployment on ASICs.\n",
      "\n",
      "I should also consider the applications beyond NLP and vision. Transformers might be used in reinforcement learning, graph neural networks, or even in recommendation systems. Each of these areas could have their own set of recent papers exploring how transformers can be adapted or improved for specific tasks.\n",
      "\n",
      "I'm a bit confused about how to search effectively. Maybe I can use Google Scholar and set the publication date filter to 2023 or 2024. Using keywords like \"transformer architecture,\" \"efficient transformers,\" \"vision transformers,\" \"model distillation,\" etc., could help narrow down the results. I should also look for survey papers or review articles that summarize recent trends in transformer research.\n",
      "\n",
      "Wait, the user provided two papers. The first one from 2022 is about Arch-Net, which uses model distillation to make transformers deployable on various ASICs. The second one from 2024 is about adversarial attacks on transformer-based trackers. So, the latest would be 2024, but I need to find other papers beyond these.\n",
      "\n",
      "I think I should also check the references of these papers to see what other recent work they cite. That might lead me to more up-to-date research. Additionally, looking at the authors' other publications could help, as they might have published follow-up studies.\n",
      "\n",
      "Another thought: maybe there are preprints on arXiv that haven't been published in conferences yet. These could be the very latest developments. I should search there with the same keywords. Also, checking the proceedings of recent conferences (like NeurIPS 2023 or ICML 2024) could yield the newest papers.\n",
      "\n",
      "I'm also wondering about the trends. Are researchers focusing on scaling up transformers, making them more efficient, or applying them to new domains? Each of these directions would have its own set of papers. For example, scaling might involve methods to handle longer sequences or larger models, while efficiency could relate to reducing computational complexity.\n",
      "\n",
      "I should also consider any recent workshops or special sessions at conferences dedicated to transformers. These might have collections of papers that I can browse through. Maybe something like the Transformer Workshop at NeurIPS or similar events.\n",
      "\n",
      "In summary, my approach should be:\n",
      "1. Use academic search engines like Google Scholar and arXiv with specific keywords and date filters.\n",
      "2. Check recent conference proceedings from top AI/ML conferences.\n",
      "3. Look at the references and citations of the provided papers to find related work.\n",
      "4. Explore different application areas beyond NLP and vision to find transformer adaptations.\n",
      "5. Consider efficiency and deployment-focused research as a key area.\n",
      "\n",
      "I might also need to create a Google Scholar alert for new papers matching my keywords to stay updated. Additionally, following researchers or institutions known for transformer research on platforms like Twitter or ResearchGate could provide leads on the latest publications.\n",
      "\n",
      "I should be careful to evaluate the credibility of the sources. Peer-reviewed conferences and journals are more reliable than preprints, though arXiv can have high-quality papers that are pending review. I'll need to assess each paper's methodology and results critically.\n",
      "\n",
      "Lastly, I might need to use tools like Semantic Scholar or Microsoft Academic to find related papers and see what's trending. These tools can sometimes suggest papers based on what I'm reading, which could help discover the latest advancements.\n",
      "</think>\n",
      "\n",
      "To find the latest papers on transformer architectures, follow this organized approach:\n",
      "\n",
      "1. **Academic Search Engines and Preprints:**\n",
      "   - Use Google Scholar with keywords like \"transformer architecture,\" \"efficient transformers,\" and set the publication date filter to 2023 or 2024.\n",
      "   - Explore arXiv.org for preprints, as they often host the latest research before formal\n",
      "\n",
      "üîó Sources:\n",
      "1. unknown: Published: 2022-04-11\n",
      "Title: Arch-Net: Model Distillation for Architecture Agnostic Model Deployment...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Query: Invalid query test 123#$%\n",
      "\n",
      "üìù Answer: <think>\n",
      "Okay, so I'm trying to understand this question about an invalid query test. The context provided is quite technical, talking about red teaming, adversarial dialogue datasets, and different methods like zero-shot generation, RL fine-tuning, etc. It mentions something about a trade-off between sample diversity and attack success rate. There's also a figure referenced, Fig. 14, which plots attack success rate against sample diversity, using metrics like self-BLEU.\n",
      "\n",
      "Hmm, the question is \"Invalid query test 123#$%\". That seems like a random string with some special characters. Maybe it's testing how the system handles invalid or nonsensical input. The context talks about testing models with adversarial examples, so perhaps this is an example of that. The model should probably handle it gracefully, maybe by returning an error message or a request for clarification.\n",
      "\n",
      "Wait, but the context also discusses methods to generate test cases to trick models into giving offensive responses. So, maybe \"Invalid query test 123#$%\" is an example of such a test case. The context mentions using nucleus sampling with p=0.95 to generate diverse samples. They measure diversity with self-BLEU, where lower is better. So, if the model is being tested with such queries, it's to see if it can maintain diversity without compromising on the success rate of attacks.\n",
      "\n",
      "I'm a bit confused about the exact purpose here. Is the question itself an invalid query, or is it part of a test case? The context seems to be about generating such test cases to evaluate model robustness. So, perhaps the question is an example of an invalid query used in testing, and the answer should explain how such queries are generated and their purpose in red teaming.\n",
      "\n",
      "Also, the context mentions Casper et al. (2023) using a human-in-the-loop process, which involves collecting human labels to train a classifier. So, maybe this invalid query is part of that process, where human evaluators label such queries to improve the model's detection of harmful content.\n",
      "\n",
      "I'm still not entirely sure how to structure the answer. Maybe I should explain that the invalid query is a test case used in adversarial testing to evaluate model robustness, and that such queries are generated using methods like zero-shot prompts and nucleus sampling to ensure diversity and effectiveness in attacking the model's offensive response classification.\n",
      "\n",
      "Wait, but the question is just \"Invalid query test 123#$%\". So perhaps the answer should be a brief explanation of what this means in the context of the provided information. It's an example of an invalid or adversarial query used to test model responses, generated through methods discussed to balance diversity and attack success.\n",
      "\n",
      "I think I need to tie it back to the trade-off mentioned. Zero-shot methods have lower attack success but better diversity, while RL fine-tuning can increase success rates but reduce diversity. So, this query might be part of a diverse set generated to test the model's vulnerabilities without overexploiting a single pattern.\n",
      "\n",
      "In summary, the invalid query is a test case used in red teaming to assess how well a model handles nonsensical or adversarial input, generated using specific methods to balance diversity and effectiveness in triggering offensive responses.\n",
      "</think>\n",
      "\n",
      "The query \"Invalid query test 123#$%\" is an example of an adversarial test case used in red teaming to evaluate a model's robustness against nonsensical or invalid input. Such queries are generated using methods like zero-shot prompts and nucleus sampling to create diverse test cases. These methods aim to balance the trade-off between sample diversity and attack success rate, ensuring that the model is tested comprehensively without overexploiting a single pattern. This approach helps assess how well the model handles unexpected or adversarial input, contributing to the evaluation of its robustness and ability to avoid generating offensive responses.\n",
      "\n",
      "üîó Sources:\n",
      "1. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases usi...\n",
      "2. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases usi...\n",
      "3. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/: The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases usi...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [1. Install Dependencies]\n",
    "!pip install -q langchain langgraph chromadb langchain-community tiktoken langchain-groq wikipedia arxiv python-dotenv\n",
    "\n",
    "# %% [2. Environment Setup]\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
    "\n",
    "# %% [3. Document Loading & Indexing]\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load documents\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {url}: {str(e)}\")\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(f\"Successfully indexed {len(doc_splits)} document chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"Indexing failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# %% [4. Initialize Retriever]\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# %% [5. Tool Setup]\n",
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "\n",
    "# Wikipedia tool\n",
    "wiki = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=2,\n",
    "        doc_content_chars_max=2000,\n",
    "        load_all_available_meta=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# ArXiv tool\n",
    "arxiv = ArxivQueryRun(\n",
    "    api_wrapper=ArxivAPIWrapper(\n",
    "        top_k_results=2,\n",
    "        doc_content_chars_max=2000\n",
    "    )\n",
    ")\n",
    "\n",
    "# %% [6. Router Setup]\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Router model\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"wikipedia\", \"arxiv\"] = Field(\n",
    "        description=\"Source to answer the question\"\n",
    "    )\n",
    "\n",
    "# LLM setup\n",
    "llm = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# Router chain\n",
    "# %% [Complete Fixed Implementation]\n",
    "from typing import TypedDict, List, Optional\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Updated state definition\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "    errors: List[str]\n",
    "    routing_decision: Optional[str]\n",
    "\n",
    "# Initialize graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes with proper state returns\n",
    "def retrieve(state: AgentState):\n",
    "    try:\n",
    "        docs = retriever.invoke(state[\"question\"])\n",
    "        return {\n",
    "            \"documents\": docs,\n",
    "            \"errors\": [],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"documents\": [],\n",
    "            \"errors\": [f\"Retrieval error: {str(e)}\"],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "\n",
    "def wiki_search(state: AgentState):\n",
    "    try:\n",
    "        content = wiki.invoke({\"query\": state[\"question\"]})\n",
    "        return {\n",
    "            \"documents\": [Document(page_content=content)],\n",
    "            \"errors\": [],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"documents\": [],\n",
    "            \"errors\": [f\"Wikipedia error: {str(e)}\"],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "\n",
    "def arxiv_search(state: AgentState):\n",
    "    try:\n",
    "        content = arxiv.invoke({\"query\": state[\"question\"]})\n",
    "        return {\n",
    "            \"documents\": [Document(page_content=content)],\n",
    "            \"errors\": [],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"documents\": [],\n",
    "            \"errors\": [f\"ArXiv error: {str(e)}\"],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "\n",
    "def generate_response(state: AgentState):\n",
    "    try:\n",
    "        context = \"\\n\".join(doc.page_content for doc in state[\"documents\"])\n",
    "        prompt = f\"Question: {state['question']}\\nContext: {context}\"\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\n",
    "            \"generation\": response.content,\n",
    "            \"errors\": state[\"errors\"],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"generation\": \"\",\n",
    "            \"errors\": state[\"errors\"] + [f\"Generation error: {str(e)}\"],\n",
    "            \"routing_decision\": state.get(\"routing_decision\")\n",
    "        }\n",
    "\n",
    "def route_question(state: AgentState):\n",
    "    try:\n",
    "        route = router_chain.invoke({\"question\": state[\"question\"]})\n",
    "        return {\n",
    "            \"question\": state[\"question\"],\n",
    "            \"documents\": [],\n",
    "            \"generation\": \"\",\n",
    "            \"errors\": [],\n",
    "            \"routing_decision\": route.datasource\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": state[\"question\"],\n",
    "            \"documents\": [],\n",
    "            \"generation\": \"\",\n",
    "            \"errors\": [f\"Routing error: {str(e)}\"],\n",
    "            \"routing_decision\": \"vectorstore\"\n",
    "        }\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"router\", route_question)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"wiki_search\", wiki_search)\n",
    "workflow.add_node(\"arxiv_search\", arxiv_search)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state.get(\"routing_decision\", \"vectorstore\"),\n",
    "    {\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"wikipedia\": \"wiki_search\",\n",
    "        \"arxiv\": \"arxiv_search\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"wiki_search\", \"generate\")\n",
    "workflow.add_edge(\"arxiv_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# %% [Fixed Test Function]\n",
    "def run_query(question):\n",
    "    print(f\"\\nüîç Query: {question}\")\n",
    "    try:\n",
    "        # Initialize with empty state\n",
    "        result = app.invoke({\n",
    "            \"question\": question,\n",
    "            \"documents\": [],\n",
    "            \"generation\": \"\",\n",
    "            \"errors\": [],\n",
    "            \"routing_decision\": None\n",
    "        })\n",
    "        \n",
    "        print(\"\\nüìù Answer:\", result.get(\"generation\", \"No answer generated\"))\n",
    "        \n",
    "        if result.get(\"documents\"):\n",
    "            print(\"\\nüîó Sources:\")\n",
    "            for i, doc in enumerate(result[\"documents\"], 1):\n",
    "                source = doc.metadata.get(\"source\", \"unknown\")\n",
    "                print(f\"{i}. {source}: {doc.page_content[:100]}...\")\n",
    "        \n",
    "        if result.get(\"errors\"):\n",
    "            print(\"\\n‚ö†Ô∏è Errors:\")\n",
    "            for error in result[\"errors\"]:\n",
    "                print(f\"- {error}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå System error: {str(e)}\")\n",
    "\n",
    "# Test cases\n",
    "test_queries = [\n",
    "    \"What is chain-of-thought prompting?\",\n",
    "    \"Explain LLM adversarial attacks\",\n",
    "    \"Who is Yoshua Bengio?\",\n",
    "    \"Latest papers on transformer architectures\",\n",
    "    \"Invalid query test 123#$%\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    run_query(query)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
